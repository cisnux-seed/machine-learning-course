{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNq2I2ICVl80Rcqh+HNZeAO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cisnux-seed/machine-learning-course/blob/main/week_14/lenet_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kode yang Anda tunjukkan adalah impor modul dan pustaka yang umumnya digunakan dalam pengembangan dengan framework PyTorch untuk pembelajaran mesin dan komputasi tensor. Berikut adalah penjelasan singkat dari setiap baris kode:\n",
        "\n",
        "1. `import torch`: Ini mengimpor modul utama dari PyTorch yang menyediakan fungsionalitas dasar seperti operasi tensor, fungsi matematika, dan komputasi GPU.\n",
        "\n",
        "2. `import torch.nn as nn`: PyTorch menyediakan modul `torch.nn` untuk membangun dan melatih jaringan saraf (neural networks). `nn` berisi kelas-kelas yang memungkinkan pembuatan berbagai jenis layer neural network seperti layer linier, aktivasi, konvolusi, dll.\n",
        "\n",
        "3. `import torch.optim as optim`: Modul `torch.optim` berisi berbagai algoritma optimisasi yang digunakan untuk melatih jaringan saraf. Ini termasuk SGD (Stochastic Gradient Descent), Adam, RMSProp, dan lainnya.\n",
        "\n",
        "4. `import torchvision`: `torchvision` adalah sub-pustaka dari PyTorch yang berfokus pada komputer vision (pengolahan gambar). Ini menyediakan utilitas untuk bekerja dengan dataset gambar, transformasi, dan model-model terkenal dalam vision.\n",
        "\n",
        "5. `import torchvision.transforms as transforms`: Modul ini berisi fungsi-fungsi yang memungkinkan transformasi pada data gambar, seperti rotasi, cropping, scaling, dan normalisasi.\n",
        "\n",
        "6. `from torch.utils.data import DataLoader`: `DataLoader` adalah kelas dari PyTorch yang digunakan untuk memuat dataset dan mengirimkan batch-batch data ke model. Ini memfasilitasi proses pelatihan dengan membagi dataset ke dalam batch-batch yang dapat ditangani oleh model secara efisien.\n",
        "\n",
        "Dengan menggabungkan fungsionalitas dari modul-modul yang diimpor dalam kode tersebut, pengembang dapat membangun, melatih, dan mengevaluasi model neural network untuk berbagai tugas, terutama dalam konteks pengolahan gambar menggunakan PyTorch."
      ],
      "metadata": {
        "id": "youSRiUPF_6b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3vQsBpboD9t8"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Baris kode yang Anda tunjukkan adalah cara untuk menentukan perangkat (device) yang akan digunakan untuk komputasi saat menggunakan PyTorch. Lebih khusus lagi, kode tersebut bertujuan untuk memilih antara menggunakan unit pemrosesan grafis (GPU) jika tersedia atau menggunakan CPU jika tidak ada GPU yang tersedia.\n",
        "\n",
        "Mari kita jelaskan lebih rinci:\n",
        "\n",
        "1. `torch.cuda.is_available()`: Ini adalah fungsi yang disediakan oleh PyTorch untuk memeriksa ketersediaan GPU. Jika PyTorch mendeteksi bahwa GPU tersedia, fungsi ini akan mengembalikan nilai `True`; jika tidak, akan mengembalikan nilai `False`.\n",
        "\n",
        "2. `\"cuda\" if torch.cuda.is_available() else \"cpu\"`: Ini adalah ekspresi kondisional Python yang menggunakan hasil dari `torch.cuda.is_available()` untuk memilih perangkat yang akan digunakan. Jika GPU tersedia, string `\"cuda\"` akan dipilih sebagai perangkat; jika tidak, maka string `\"cpu\"` akan dipilih sebagai perangkat.\n",
        "\n",
        "3. `device = torch.device(...)`: Baris kode ini menggunakan nilai yang dipilih sebelumnya (\"cuda\" atau \"cpu\") untuk menetapkan perangkat yang akan digunakan dalam konteks komputasi PyTorch. Dalam hal ini, variabel `device` akan mewakili perangkat yang akan digunakan selama operasi PyTorch.\n",
        "\n",
        "Jadi, keseluruhan baris kode tersebut bertujuan untuk menentukan perangkat (GPU atau CPU) yang akan digunakan dalam eksekusi operasi PyTorch selanjutnya berdasarkan ketersediaan GPU pada sistem yang menjalankan kode tersebut. Hal ini memungkinkan fleksibilitas dalam penggunaan perangkat keras yang tersedia untuk melakukan komputasi yang lebih cepat, terutama dalam kasus pelatihan model deep learning di GPU yang memiliki kemampuan komputasi yang lebih besar daripada CPU."
      ],
      "metadata": {
        "id": "jv_3FD0nGJ2Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set device to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "9lxujkm2EodF"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kode yang Anda tunjukkan adalah implementasi dari arsitektur jaringan neural LeNet-5 menggunakan modul `nn.Module` dari PyTorch. Arsitektur LeNet-5 adalah salah satu arsitektur jaringan saraf konvolusional (CNN) yang pertama kali diperkenalkan oleh Yann LeCun, Leon Bottou, Yoshua Bengio, dan Patrick Haffner pada tahun 1998 untuk mengenali digit angka tulisan tangan pada dataset MNIST.\n",
        "\n",
        "Mari kita jelaskan kode tersebut baris per baris:\n",
        "\n",
        "1. `class LeNet5(nn.Module):`: Ini adalah definisi kelas yang disebut `LeNet5`. Kelas ini adalah turunan dari `nn.Module` di PyTorch, yang berarti kelas ini akan menjadi model neural network.\n",
        "\n",
        "2. `def __init__(self):`: Metode `__init__` di dalam kelas `LeNet5` ini digunakan untuk mendefinisikan struktur model.\n",
        "\n",
        "   a. `self.conv1 = nn.Conv2d(1, 6, kernel_size=5, stride=1, padding=2)`: Ini adalah layer konvolusi pertama dengan input 1 channel (greyscale untuk MNIST), output 6 channel, filter/kernel size 5x5, dengan stride 1, dan padding 2.\n",
        "   \n",
        "   b. `self.relu1 = nn.ReLU()`: Activation function ReLU setelah konvolusi pertama.\n",
        "   \n",
        "   c. `self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)`: Layer max pooling dengan kernel size 2x2 dan stride 2 setelah ReLU pertama.\n",
        "\n",
        "   d. `self.conv2 = nn.Conv2d(6, 16, kernel_size=5, stride=1)`: Layer konvolusi kedua dengan input 6 channel dan output 16 channel, filter/kernel size 5x5, dan stride 1.\n",
        "\n",
        "   e. `self.relu2 = nn.ReLU()`: Activation function ReLU setelah konvolusi kedua.\n",
        "\n",
        "   f. `self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)`: Layer max pooling kedua dengan kernel size 2x2 dan stride 2 setelah ReLU kedua.\n",
        "\n",
        "   g. `self.fc1 = nn.Linear(16 * 5 * 5, 120)`: Layer fully connected pertama dengan 16x5x5 input features dan 120 output features.\n",
        "\n",
        "   h. `self.relu3 = nn.ReLU()`: Activation function ReLU setelah fully connected layer pertama.\n",
        "\n",
        "   i. `self.fc2 = nn.Linear(120, 84)`: Layer fully connected kedua dengan 120 input features dan 84 output features.\n",
        "\n",
        "   j. `self.relu4 = nn.ReLU()`: Activation function ReLU setelah fully connected layer kedua.\n",
        "\n",
        "   k. `self.fc3 = nn.Linear(84, 10)`: Layer fully connected ketiga dengan 84 input features (dari layer sebelumnya) dan 10 output features (sesuai dengan jumlah kelas pada dataset MNIST).\n",
        "\n",
        "3. `def forward(self, x):`: Metode `forward` mendefinisikan aliran maju (forward pass) melalui jaringan. Operasi ini terjadi saat input data diteruskan melalui model.\n",
        "\n",
        "   a. `x = self.pool1(self.relu1(self.conv1(x)))`: Melakukan konvolusi pertama, aktivasi ReLU, dan max pooling.\n",
        "\n",
        "   b. `x = self.pool2(self.relu2(self.conv2(x)))`: Melakukan konvolusi kedua, aktivasi ReLU, dan max pooling lagi.\n",
        "\n",
        "   c. `x = x.view(-1, 16 * 5 * 5)`: Mengubah bentuk tensor untuk disesuaikan dengan fully connected layer.\n",
        "\n",
        "   d. `x = self.relu3(self.fc1(x))`: Melakukan operasi fully connected pertama dan aktivasi ReLU.\n",
        "\n",
        "   e. `x = self.relu4(self.fc2(x))`: Melakukan operasi fully connected kedua dan aktivasi ReLU.\n",
        "\n",
        "   f. `x = self.fc3(x)`: Melakukan operasi fully connected ketiga tanpa aktivasi untuk mendapatkan output prediksi.\n",
        "\n",
        "4. `return x`: Mengembalikan hasil prediksi dari jaringan.\n",
        "\n",
        "Dengan demikian, ini adalah implementasi dari arsitektur LeNet-5 dalam PyTorch, siap untuk digunakan untuk melakukan klasifikasi pada dataset MNIST atau tugas pengenalan pola serupa."
      ],
      "metadata": {
        "id": "N-PF-_2OGUY4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the LeNet-5 architecture for MNIST\n",
        "class LeNet5(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet5, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5, stride=1, padding=2)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5, stride=1)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.relu3 = nn.ReLU()\n",
        "\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.relu4 = nn.ReLU()\n",
        "\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(self.relu1(self.conv1(x)))\n",
        "        x = self.pool2(self.relu2(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = self.relu3(self.fc1(x))\n",
        "        x = self.relu4(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "pybuDDzWEygc"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kode yang Anda tunjukkan adalah proses untuk memuat dataset MNIST menggunakan PyTorch. Dataset MNIST merupakan dataset yang sering digunakan dalam pengembangan dan pengujian model pada bidang computer vision untuk mengenali angka tulisan tangan dari gambar berukuran 28x28 piksel.\n",
        "\n",
        "Mari kita jelaskan baris per baris:\n",
        "\n",
        "1. `transform = transforms.Compose([...])`: Ini adalah proses transformasi data yang akan diterapkan pada setiap sampel dari dataset MNIST sebelum dimasukkan ke dalam model. Transformasi yang dilakukan adalah:\n",
        "   - `transforms.Resize((28, 28))`: Mengubah ukuran gambar menjadi 28x28 piksel.\n",
        "   - `transforms.ToTensor()`: Mengubah gambar menjadi tensor PyTorch.\n",
        "\n",
        "2. `train_dataset = torchvision.datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)`: Ini adalah pengaturan dataset pelatihan. Fungsi `torchvision.datasets.MNIST` digunakan untuk mengunduh dataset MNIST jika belum ada, atau jika sudah ada, akan memuatnya dari direktori yang telah ditentukan (`root=\"./data\"`). `train=True` menunjukkan bahwa dataset pelatihan akan dimuat. Transformasi yang telah ditentukan sebelumnya akan diterapkan pada dataset ini.\n",
        "\n",
        "3. `test_dataset = torchvision.datasets.MNIST(root=\"./data\", train=False, download=True, transform=transform)`: Ini adalah pengaturan dataset pengujian. Sama seperti sebelumnya, namun dengan `train=False`, yang berarti dataset pengujian yang akan dimuat.\n",
        "\n",
        "4. `train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)`: `DataLoader` digunakan untuk memuat dataset dan membaginya menjadi batch-batch yang akan digunakan untuk pelatihan model. `train_loader` adalah loader untuk dataset pelatihan. Parameter `batch_size=64` menunjukkan bahwa setiap batch akan berisi 64 sampel. `shuffle=True` menyebabkan pengacakan data pada setiap epoch (perulangan) selama pelatihan.\n",
        "\n",
        "5. `test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)`: `test_loader` adalah loader untuk dataset pengujian. Sama seperti sebelumnya, namun dengan `shuffle=False`, sehingga data pengujian tidak diacak.\n",
        "\n",
        "Dengan menggunakan kode ini, dataset MNIST telah dimuat dan dipersiapkan dalam format yang sesuai untuk digunakan dalam pelatihan dan evaluasi model neural network pada tugas pengenalan angka tulisan tangan. DataLoader akan memudahkan penggunaan dataset ini saat melatih model, mengelola batch-batch data untuk iterasi selama proses pelatihan."
      ],
      "metadata": {
        "id": "aIJKPmZpGbbq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load MNIST dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((28, 28)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "train_dataset = torchvision.datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
        "test_dataset = torchvision.datasets.MNIST(root=\"./data\", train=False, download=True, transform=transform)\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "id": "vyHbweQHE7Ti"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kode yang Anda tunjukkan adalah proses pelatihan (training) dari model neural network menggunakan dataset MNIST yang telah dimuat sebelumnya. Ini adalah tahap di mana model benar-benar belajar dari data untuk menyesuaikan parameter-parameter internalnya.\n",
        "\n",
        "Mari kita jelaskan baris per baris:\n",
        "\n",
        "1. `model = LeNet5().to(device)`: Inisialisasi model LeNet-5 yang telah didefinisikan sebelumnya. `LeNet5()` membuat instance dari kelas `LeNet5` yang merupakan model neural network untuk pengenalan angka MNIST. `.to(device)` digunakan untuk memindahkan model ke perangkat yang telah ditentukan sebelumnya (CPU atau GPU).\n",
        "\n",
        "2. `criterion = nn.CrossEntropyLoss()`: Inisialisasi fungsi kerugian (loss function) untuk perhitungan kesalahan (error) selama pelatihan. `nn.CrossEntropyLoss()` digunakan khusus untuk masalah klasifikasi dengan banyak kelas. Loss function ini akan menghitung kesalahan antara prediksi model dan label yang sebenarnya.\n",
        "\n",
        "3. `optimizer = optim.Adam(model.parameters(), lr=0.001)`: Inisialisasi optimizer, dalam hal ini menggunakan algoritma Adam untuk mengoptimalkan parameter-parameter dari model. `model.parameters()` memberikan parameter-parameter yang harus dioptimalkan. `lr=0.001` adalah laju pembelajaran (learning rate) yang digunakan oleh algoritma Adam untuk menyesuaikan parameter.\n",
        "\n",
        "4. `num_epochs = 5`: Jumlah iterasi atau epoch (putaran) pelatihan yang akan dilakukan.\n",
        "\n",
        "5. Loop `for epoch in range(num_epochs):`: Ini adalah loop yang akan melatih model selama sejumlah epoch yang telah ditentukan.\n",
        "\n",
        "   a. `model.train()`: Mengatur model dalam mode pelatihan. Ini penting karena beberapa lapisan dalam model dapat berperilaku berbeda antara mode pelatihan dan evaluasi (mode inferensi).\n",
        "\n",
        "   b. `total_loss = 0.0`: Inisialisasi total loss untuk setiap epoch.\n",
        "\n",
        "   c. Loop `for inputs, labels in train_loader:`: Ini adalah loop yang mengiterasi melalui setiap batch dalam `train_loader` yang telah dibuat sebelumnya.\n",
        "\n",
        "      - `inputs, labels = inputs.to(device), labels.to(device)`: Memindahkan data masukan (inputs) dan label ke perangkat yang telah ditentukan sebelumnya (GPU atau CPU).\n",
        "\n",
        "      - `optimizer.zero_grad()`: Mengatur gradien parameter ke nol sebelum melakukan perhitungan gradien dalam setiap iterasi. Hal ini penting karena PyTorch akan mengakumulasi gradien pada setiap iterasi jika tidak diatur ke nol.\n",
        "\n",
        "      - `outputs = model(inputs)`: Mendapatkan prediksi dari model untuk batch saat ini.\n",
        "\n",
        "      - `loss = criterion(outputs, labels)`: Menghitung loss atau kesalahan antara prediksi model dan label yang sebenarnya.\n",
        "\n",
        "      - `loss.backward()`: Melakukan backpropagation untuk menghitung gradien loss function terhadap parameter-parameter model.\n",
        "\n",
        "      - `optimizer.step()`: Melakukan langkah optimisasi, yaitu memperbarui parameter-parameter model berdasarkan gradien yang dihitung sebelumnya.\n",
        "\n",
        "      - `total_loss += loss.item()`: Menambahkan loss dari batch saat ini ke total_loss untuk menghitung rata-rata loss per epoch.\n",
        "\n",
        "   d. `print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss / len(train_loader)}\")`: Menampilkan rata-rata loss untuk setiap epoch. Loss ini adalah total_loss dibagi dengan jumlah batch dalam `train_loader`.\n",
        "\n",
        "Proses ini adalah inti dari pelatihan model, di mana model secara bertahap memperbarui parameter-parameter internalnya untuk meningkatkan akurasi prediksi terhadap dataset pelatihan. Setelah semua epoch selesai dilatih, model dapat dievaluasi terhadap dataset pengujian (test set) untuk melihat kinerjanya pada data yang belum pernah dilihat sebelumnya."
      ],
      "metadata": {
        "id": "e6IlzraCGoGl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model, loss function, and optimizer\n",
        "model = LeNet5().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training\n",
        "num_epochs = 5\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss / len(train_loader)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wlKHBnjXE-kw",
        "outputId": "c9f5fee1-acbd-4e3c-f1d9-ddf88e893ae6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5, Loss: 0.3132841411823117\n",
            "Epoch 2/5, Loss: 0.08717844293548116\n",
            "Epoch 3/5, Loss: 0.06198321758279962\n",
            "Epoch 4/5, Loss: 0.048499483921712064\n",
            "Epoch 5/5, Loss: 0.04143670365462072\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kode yang Anda tunjukkan adalah proses evaluasi model yang telah dilatih sebelumnya menggunakan dataset pengujian (test set) untuk menghitung akurasi prediksi model pada data yang belum pernah dilihat sebelumnya.\n",
        "\n",
        "Mari kita jelaskan baris per baris:\n",
        "\n",
        "1. `model.eval()`: Menempatkan model ke dalam mode evaluasi. Saat model berada dalam mode evaluasi, lapisan-lapisan yang berperilaku berbeda antara mode pelatihan dan evaluasi (seperti dropout atau batch normalization) akan beroperasi dengan cara yang berbeda.\n",
        "\n",
        "2. `total_correct = 0` dan `total_samples = 0`: Variabel ini digunakan untuk menghitung jumlah prediksi yang benar dan jumlah total sampel yang dievaluasi.\n",
        "\n",
        "3. `with torch.no_grad():`: Ini adalah blok kode yang menunjukkan bahwa dalam evaluasi ini, tidak perlu melacak gradien. Hal ini dilakukan untuk menghemat memori dan waktu komputasi karena gradien tidak diperlukan saat melakukan evaluasi.\n",
        "\n",
        "4. Loop `for inputs, labels in test_loader:`: Ini adalah loop yang mengiterasi melalui setiap batch dalam `test_loader`.\n",
        "\n",
        "   a. `inputs, labels = inputs.to(device), labels.to(device)`: Memindahkan data masukan (inputs) dan label ke perangkat yang telah ditentukan sebelumnya (GPU atau CPU).\n",
        "\n",
        "   b. `outputs = model(inputs)`: Mendapatkan prediksi dari model untuk batch saat ini.\n",
        "\n",
        "   c. `_, predictions = torch.max(outputs, 1)`: Menggunakan `torch.max()` untuk mendapatkan indeks kelas yang memiliki probabilitas tertinggi. Dalam kasus ini, yang diambil adalah indeks dari kelas dengan nilai probabilitas terbesar.\n",
        "\n",
        "   d. `(predictions == labels).sum().item()`: Menghitung jumlah prediksi yang benar dengan membandingkan prediksi model dengan label yang sebenarnya, kemudian menjumlahkannya. `.sum()` digunakan untuk menghitung jumlah elemen True (prediksi yang benar), dan `.item()` mengambil nilai hasilnya.\n",
        "\n",
        "   e. `total_correct += (predictions == labels).sum().item()`: Menambahkan jumlah prediksi yang benar dari batch saat ini ke total_correct.\n",
        "\n",
        "   f. `total_samples += labels.size(0)`: Menambahkan jumlah sampel dalam batch saat ini ke total_samples.\n",
        "\n",
        "5. `accuracy = total_correct / total_samples`: Menghitung akurasi model dengan membagi jumlah prediksi yang benar dengan jumlah total sampel yang dievaluasi.\n",
        "\n",
        "6. `print(f\"Test Accuracy: {accuracy}\")`: Mencetak akurasi model pada dataset pengujian.\n",
        "\n",
        "Proses ini adalah tahap evaluasi yang penting setelah pelatihan model. Melalui proses ini, Anda dapat mengukur seberapa baik model Anda dapat melakukan prediksi pada data yang belum pernah dilihat sebelumnya (data pengujian), memberikan gambaran tentang kinerja model secara keseluruhan. Dengan mendapatkan nilai akurasi, Anda dapat mengevaluasi sejauh mana model dapat menggeneralisasi pada data baru yang tidak pernah dilihat selama pelatihan."
      ],
      "metadata": {
        "id": "qoLdRUPmGp1c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing\n",
        "model.eval()\n",
        "total_correct = 0\n",
        "total_samples = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        _, predictions = torch.max(outputs, 1)\n",
        "\n",
        "        total_correct += (predictions == labels).sum().item()\n",
        "        total_samples += labels.size(0)\n",
        "\n",
        "accuracy = total_correct / total_samples\n",
        "print(f\"Test Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7kIdISzpFUdQ",
        "outputId": "48f4bb90-0789-4137-9fec-ef58d783276b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.9837\n"
          ]
        }
      ]
    }
  ]
}