{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPrdrFvUzsTSmvyDRN/awE/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cisnux-seed/machine-learning-course/blob/main/week_14/alex_net.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Baris kode yang Anda tunjukkan adalah impor modul dan pustaka yang umumnya digunakan dalam pengembangan menggunakan framework PyTorch untuk pembelajaran mesin dan komputasi tensor. Berikut adalah penjelasan singkat dari setiap baris kode:\n",
        "\n",
        "1. `import torch`: Ini mengimpor modul utama dari PyTorch yang menyediakan fungsionalitas dasar seperti operasi tensor, fungsi matematika, dan komputasi GPU.\n",
        "\n",
        "2. `import torch.nn as nn`: PyTorch menyediakan modul `torch.nn` untuk membangun dan melatih jaringan saraf (neural networks). `nn` berisi kelas-kelas yang memungkinkan pembuatan berbagai jenis layer neural network seperti layer linier, aktivasi, konvolusi, dll.\n",
        "\n",
        "3. `import torch.optim as optim`: Modul `torch.optim` berisi berbagai algoritma optimisasi yang digunakan untuk melatih jaringan saraf. Ini termasuk SGD (Stochastic Gradient Descent), Adam, RMSProp, dan lainnya.\n",
        "\n",
        "4. `import torchvision`: `torchvision` adalah sub-pustaka dari PyTorch yang berfokus pada komputer vision (pengolahan gambar). Ini menyediakan utilitas untuk bekerja dengan dataset gambar, transformasi, dan model-model terkenal dalam vision.\n",
        "\n",
        "5. `import torchvision.transforms as transforms`: Modul ini berisi fungsi-fungsi yang memungkinkan transformasi pada data gambar, seperti rotasi, cropping, scaling, dan normalisasi.\n",
        "\n",
        "6. `from torch.utils.data import DataLoader`: `DataLoader` adalah kelas dari PyTorch yang digunakan untuk memuat dataset dan mengirimkan batch-batch data ke model. Ini memfasilitasi proses pelatihan dengan membagi dataset ke dalam batch-batch yang dapat ditangani oleh model secara efisien.\n",
        "\n",
        "Dengan menggunakan impor dari modul-modul ini, Anda dapat membangun, melatih, dan mengevaluasi model-model neural network menggunakan PyTorch untuk berbagai macam tugas, terutama dalam konteks pengolahan gambar atau vision, namun juga dapat digunakan untuk berbagai tugas pembelajaran mesin lainnya."
      ],
      "metadata": {
        "id": "1RK2W3cxNkXw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "z8iJ2HAxHPq_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Baris kode ini bertujuan untuk menentukan perangkat yang akan digunakan untuk komputasi saat menggunakan PyTorch. Lebih khusus lagi, baris kode ini akan menetapkan perangkat ke GPU jika GPU tersedia, dan jika tidak tersedia, akan menggunakan CPU sebagai perangkat komputasi.\n",
        "\n",
        "Mari kita jelaskan lebih rinci:\n",
        "\n",
        "1. `torch.cuda.is_available()`: Ini adalah fungsi yang disediakan oleh PyTorch untuk memeriksa ketersediaan GPU. Jika GPU tersedia pada sistem, fungsi ini akan mengembalikan nilai `True`; jika tidak, akan mengembalikan nilai `False`.\n",
        "\n",
        "2. `\"cuda\" if torch.cuda.is_available() else \"cpu\"`: Ini adalah ekspresi kondisional Python yang menggunakan hasil dari `torch.cuda.is_available()` untuk memilih perangkat yang akan digunakan. Jika GPU tersedia, string `\"cuda\"` akan dipilih sebagai perangkat; jika tidak, string `\"cpu\"` akan dipilih sebagai perangkat.\n",
        "\n",
        "3. `device = torch.device(...)`: Baris kode ini menggunakan nilai yang dipilih sebelumnya (\"cuda\" atau \"cpu\") untuk menetapkan perangkat yang akan digunakan dalam konteks komputasi PyTorch. Variabel `device` akan mewakili perangkat yang akan digunakan selama operasi PyTorch, memungkinkan penggunaan GPU jika tersedia atau beralih ke CPU jika tidak tersedia.\n",
        "\n",
        "Pemilihan perangkat yang tepat untuk komputasi sangat penting saat menggunakan PyTorch, terutama dalam konteks pembelajaran mesin yang memerlukan perhitungan yang intensif. GPU memiliki kemampuan komputasi yang lebih cepat dan lebih besar daripada CPU dalam beberapa kasus, sehingga memanfaatkan GPU ketika tersedia dapat mempercepat proses pelatihan model. Namun demikian, dengan pengecualian kemampuan komputasi, PyTorch memungkinkan penggunaan keduanya (CPU dan GPU) untuk melakukan operasi komputasi pada model."
      ],
      "metadata": {
        "id": "4XcYWSEpNwzd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set device to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "imqM681MHf3y"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kode yang Anda tunjukkan adalah definisi dari arsitektur jaringan neural AlexNet yang dimodifikasi untuk digunakan dengan dataset MNIST. AlexNet awalnya diperkenalkan oleh Alex Krizhevsky, Ilya Sutskever, dan Geoffrey Hinton pada tahun 2012 dan menjadi terkenal karena berhasil memenangkan kompetisi ImageNet Large Scale Visual Recognition Challenge (ILSVRC) pada saat itu.\n",
        "\n",
        "Mari kita jelaskan baris per baris:\n",
        "\n",
        "1. `class AlexNetMNIST(nn.Module):`: Ini adalah definisi kelas yang disebut `AlexNetMNIST`. Kelas ini adalah turunan dari `nn.Module` di PyTorch, yang berarti kelas ini akan menjadi model neural network.\n",
        "\n",
        "2. `def __init__(self):`: Metode `__init__` di dalam kelas `AlexNetMNIST` ini digunakan untuk mendefinisikan struktur model.\n",
        "\n",
        "   a. `self.features = nn.Sequential(...)`:\n",
        "      - `nn.Sequential` adalah urutan dari beberapa lapisan neural network yang dijalankan berurutan satu sama lain.\n",
        "      - Ini adalah bagian dari model yang berisi lapisan-lapisan konvolusi, aktivasi ReLU, dan max pooling. Lapisan-lapisan ini membentuk bagian fitur (feature extraction) dari model.\n",
        "\n",
        "   b. `self.avgpool = nn.AdaptiveAvgPool2d((6, 6))`:\n",
        "      - `nn.AdaptiveAvgPool2d` adalah lapisan yang menyesuaikan rata-rata pooling berdasarkan ukuran yang ditentukan.\n",
        "      - Lapisan ini mengubah ukuran keluaran dari bagian fitur menjadi (6, 6).\n",
        "\n",
        "   c. `self.classifier = nn.Sequential(...)`:\n",
        "      - Bagian ini berisi lapisan-lapisan yang terkait dengan klasifikasi.\n",
        "      - Terdiri dari lapisan-lapisan dropout, lapisan-lapisan fully connected (Linear), dan aktivasi ReLU.\n",
        "\n",
        "3. `def forward(self, x):`: Metode `forward` mendefinisikan aliran maju (forward pass) melalui jaringan. Operasi ini terjadi saat input data diteruskan melalui model.\n",
        "\n",
        "   a. `x = self.features(x)`: Melakukan operasi feature extraction menggunakan lapisan-lapisan konvolusi, aktivasi ReLU, dan max pooling yang telah didefinisikan sebelumnya.\n",
        "\n",
        "   b. `x = self.avgpool(x)`: Melakukan adaptive average pooling pada output dari bagian fitur untuk mengubah ukurannya menjadi (6, 6).\n",
        "\n",
        "   c. `x = torch.flatten(x, 1)`: Melakukan flattening dari tensor untuk disesuaikan dengan fully connected layer.\n",
        "\n",
        "   d. `x = self.classifier(x)`: Melakukan operasi klasifikasi menggunakan lapisan-lapisan fully connected yang telah didefinisikan sebelumnya.\n",
        "\n",
        "   e. `return x`: Mengembalikan output hasil dari proses forward pass.\n",
        "\n",
        "Dengan demikian, ini adalah implementasi dari arsitektur jaringan neural AlexNet yang dimodifikasi untuk digunakan dalam tugas pengenalan digit dari dataset MNIST dalam konteks PyTorch."
      ],
      "metadata": {
        "id": "N-yoEqaQNywC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the AlexNet architecture for MNIST\n",
        "class AlexNetMNIST(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AlexNetMNIST, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(1, 64, kernel_size=11, stride=4, padding=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "        )\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(256 * 6 * 6, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(4096, 10),  # 10 classes for MNIST\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "b7ASYS9-Hg4P"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kode yang Anda tunjukkan adalah proses untuk memuat dataset MNIST menggunakan PyTorch. Dataset MNIST adalah dataset standar yang sering digunakan dalam pengembangan dan pengujian model pada bidang computer vision untuk mengenali angka tulisan tangan dari gambar berukuran 28x28 piksel. Dalam kode ini, dataset MNIST diubah ukurannya menjadi 224x224 piksel dan diubah menjadi tensor PyTorch.\n",
        "\n",
        "Mari kita jelaskan baris per baris:\n",
        "\n",
        "1. `transform = transforms.Compose([...])`: Ini adalah proses transformasi data yang akan diterapkan pada setiap sampel dari dataset MNIST sebelum dimasukkan ke dalam model. Transformasi yang dilakukan adalah:\n",
        "\n",
        "   - `transforms.Resize((224, 224))`: Mengubah ukuran gambar menjadi 224x224 piksel. Dalam hal ini, gambar MNIST yang aslinya berukuran 28x28 piksel akan diperbesar menjadi ukuran yang lebih besar, agar cocok dengan masukan yang diharapkan oleh arsitektur jaringan seperti AlexNet atau model-model lain yang membutuhkan ukuran input yang lebih besar.\n",
        "\n",
        "   - `transforms.ToTensor()`: Mengubah gambar menjadi tensor PyTorch.\n",
        "\n",
        "2. `train_dataset = torchvision.datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)`: Ini adalah pengaturan dataset pelatihan. Fungsi `torchvision.datasets.MNIST` digunakan untuk mengunduh dataset MNIST jika belum ada, atau jika sudah ada, akan memuatnya dari direktori yang telah ditentukan (`root=\"./data\"`). `train=True` menunjukkan bahwa dataset pelatihan akan dimuat. Transformasi yang telah ditentukan sebelumnya akan diterapkan pada dataset ini.\n",
        "\n",
        "3. `test_dataset = torchvision.datasets.MNIST(root=\"./data\", train=False, download=True, transform=transform)`: Ini adalah pengaturan dataset pengujian. Sama seperti sebelumnya, namun dengan `train=False`, yang berarti dataset pengujian yang akan dimuat.\n",
        "\n",
        "4. `train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)`: `DataLoader` digunakan untuk memuat dataset dan membaginya menjadi batch-batch yang akan digunakan untuk pelatihan model. `train_loader` adalah loader untuk dataset pelatihan. Parameter `batch_size=64` menunjukkan bahwa setiap batch akan berisi 64 sampel. `shuffle=True` menyebabkan pengacakan data pada setiap epoch (perulangan) selama pelatihan.\n",
        "\n",
        "5. `test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)`: `test_loader` adalah loader untuk dataset pengujian. Sama seperti sebelumnya, namun dengan `shuffle=False`, sehingga data pengujian tidak diacak.\n",
        "\n",
        "Dengan menggunakan kode ini, dataset MNIST telah dimuat dan dipersiapkan dalam format yang sesuai untuk digunakan dalam pelatihan dan evaluasi model neural network pada tugas pengenalan angka tulisan tangan. DataLoader akan memudahkan penggunaan dataset ini saat melatih model, mengelola batch-batch data untuk iterasi selama proses pelatihan."
      ],
      "metadata": {
        "id": "CI-9NjhpOIX3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load MNIST dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "train_dataset = torchvision.datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
        "test_dataset = torchvision.datasets.MNIST(root=\"./data\", train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62tZIzfkHkV-",
        "outputId": "4a2b2ebd-25af-42cd-b5c9-ed11b85c662e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 106745620.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 6092118.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 32342791.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 16296431.79it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Baris kode yang Anda tunjukkan adalah proses inisialisasi model, fungsi kerugian (loss function), dan optimizer untuk pelatihan model AlexNet yang telah Anda definisikan sebelumnya untuk digunakan dengan dataset MNIST.\n",
        "\n",
        "Mari kita jelaskan baris per baris:\n",
        "\n",
        "1. `model = AlexNetMNIST().to(device)`: Ini adalah inisialisasi model `AlexNetMNIST` yang telah Anda definisikan sebelumnya. `AlexNetMNIST()` membuat instance dari kelas `AlexNetMNIST` yang merupakan model neural network yang ditujukan untuk pengenalan digit MNIST. `.to(device)` digunakan untuk memindahkan model ke perangkat yang telah ditentukan sebelumnya (CPU atau GPU) untuk melakukan komputasi.\n",
        "\n",
        "2. `criterion = nn.CrossEntropyLoss()`: Inisialisasi fungsi kerugian (loss function) untuk perhitungan kesalahan (error) selama pelatihan. `nn.CrossEntropyLoss()` digunakan khusus untuk masalah klasifikasi dengan banyak kelas seperti dalam dataset MNIST. Loss function ini akan menghitung kesalahan antara prediksi model dan label yang sebenarnya.\n",
        "\n",
        "3. `optimizer = optim.Adam(model.parameters(), lr=0.001)`: Inisialisasi optimizer, dalam hal ini menggunakan algoritma Adam untuk mengoptimalkan parameter-parameter dari model. `model.parameters()` memberikan parameter-parameter yang harus dioptimalkan. `lr=0.001` adalah laju pembelajaran (learning rate) yang digunakan oleh algoritma Adam untuk menyesuaikan parameter.\n",
        "\n",
        "Dengan kode ini, Anda telah menyiapkan model, fungsi kerugian, dan optimizer yang akan digunakan dalam proses pelatihan. Model siap untuk dilatih menggunakan data dari dataset MNIST dengan menggunakan algoritma optimisasi Adam dan fungsi kerugian Cross Entropy Loss untuk memperbarui parameter agar model dapat belajar dan meningkatkan performanya dalam mengklasifikasikan gambar-gambar digit pada dataset MNIST."
      ],
      "metadata": {
        "id": "oLkjxQsIOKt6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model, loss function, and optimizer\n",
        "model = AlexNetMNIST().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "C5gXZOwCHrBQ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kode yang Anda tunjukkan adalah proses pelatihan (training) dari model AlexNet menggunakan dataset pelatihan MNIST. Dalam kode ini, model akan dilatih dalam beberapa epoch (putaran), di mana setiap epoch terdiri dari beberapa batch data.\n",
        "\n",
        "Mari kita jelaskan baris per baris:\n",
        "\n",
        "1. `num_epochs = 5`: Variabel `num_epochs` menunjukkan jumlah putaran pelatihan yang akan dilakukan pada dataset pelatihan. Dalam hal ini, Anda telah menetapkan jumlah epoch sebanyak 5.\n",
        "\n",
        "2. `for epoch in range(num_epochs):`: Loop `for` ini akan berjalan sebanyak `num_epochs`, yaitu 5 kali dalam contoh Anda.\n",
        "\n",
        "   a. `model.train()`: Memastikan bahwa model berada dalam mode pelatihan. Beberapa lapisan, seperti dropout dan batch normalization, berperilaku berbeda antara mode pelatihan dan evaluasi. Dengan memanggil `model.train()`, model akan disetel untuk mode pelatihan.\n",
        "\n",
        "   b. `total_loss = 0.0`: Inisialisasi nilai total loss untuk setiap epoch.\n",
        "\n",
        "   c. Loop `for inputs, labels in train_loader:`: Ini adalah loop yang mengiterasi melalui setiap batch dalam `train_loader`, yang memuat dataset pelatihan dalam batch-batch kecil.\n",
        "\n",
        "      - `inputs, labels = inputs.to(device), labels.to(device)`: Memindahkan data masukan (inputs) dan label ke perangkat yang telah ditentukan sebelumnya (GPU atau CPU) untuk komputasi.\n",
        "\n",
        "      - `optimizer.zero_grad()`: Mengatur gradien parameter ke nol sebelum melakukan perhitungan gradien dalam setiap iterasi. Hal ini penting karena PyTorch akan mengakumulasi gradien pada setiap iterasi jika tidak diatur ke nol.\n",
        "\n",
        "      - `outputs = model(inputs)`: Mendapatkan prediksi dari model untuk batch saat ini.\n",
        "\n",
        "      - `loss = criterion(outputs, labels)`: Menghitung loss atau kesalahan antara prediksi model dan label yang sebenarnya.\n",
        "\n",
        "      - `loss.backward()`: Melakukan backpropagation untuk menghitung gradien loss function terhadap parameter-parameter model.\n",
        "\n",
        "      - `optimizer.step()`: Melakukan langkah optimisasi, yaitu memperbarui parameter-parameter model berdasarkan gradien yang dihitung sebelumnya.\n",
        "\n",
        "      - `total_loss += loss.item()`: Menambahkan loss dari batch saat ini ke total_loss untuk menghitung rata-rata loss per epoch.\n",
        "\n",
        "   d. `print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss / len(train_loader)}\")`: Mencetak rata-rata loss untuk setiap epoch. Loss ini adalah total_loss dibagi dengan jumlah batch dalam `train_loader`.\n",
        "\n",
        "Proses ini adalah inti dari pelatihan model neural network. Dengan menjalankan proses ini, model secara iteratif diperbarui dan disesuaikan dengan data training untuk meningkatkan performa dan kemampuannya dalam menggeneralisasi pola dari data yang diberikan. Output yang ditampilkan merupakan informasi tentang rata-rata loss yang dihasilkan oleh model pada setiap epoch selama proses pelatihan."
      ],
      "metadata": {
        "id": "HUGHiToCOYNR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training\n",
        "num_epochs = 5\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss / len(train_loader)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43SQEZbTHvWr",
        "outputId": "d081b7ed-5c73-454b-98a2-89ff978b784d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5, Loss: 0.29136410713228567\n",
            "Epoch 2/5, Loss: 0.08433905105410106\n",
            "Epoch 3/5, Loss: 0.07055862730642387\n",
            "Epoch 4/5, Loss: 0.05991470559242412\n",
            "Epoch 5/5, Loss: 0.053071164018481345\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kode yang Anda tunjukkan adalah proses evaluasi model setelah proses pelatihan selesai menggunakan dataset pengujian (test set). Tujuannya adalah untuk mengukur akurasi model pada data yang belum pernah dilihat sebelumnya.\n",
        "\n",
        "Mari kita jelaskan baris per baris:\n",
        "\n",
        "1. `model.eval()`: Memastikan bahwa model berada dalam mode evaluasi. Beberapa lapisan, seperti dropout dan batch normalization, berperilaku berbeda antara mode pelatihan dan evaluasi. Dengan memanggil `model.eval()`, model akan disetel untuk mode evaluasi.\n",
        "\n",
        "2. `total_correct = 0` dan `total_samples = 0`: Variabel ini digunakan untuk menghitung jumlah prediksi yang benar dan jumlah total sampel yang dievaluasi.\n",
        "\n",
        "3. `with torch.no_grad():`: Ini adalah blok kode yang menunjukkan bahwa dalam evaluasi ini, tidak perlu melacak gradien. Hal ini dilakukan untuk menghemat memori dan waktu komputasi karena gradien tidak diperlukan saat melakukan evaluasi.\n",
        "\n",
        "4. Loop `for inputs, labels in test_loader:`: Ini adalah loop yang mengiterasi melalui setiap batch dalam `test_loader`, yang memuat dataset pengujian dalam batch-batch kecil.\n",
        "\n",
        "   a. `inputs, labels = inputs.to(device), labels.to(device)`: Memindahkan data masukan (inputs) dan label ke perangkat yang telah ditentukan sebelumnya (GPU atau CPU) untuk komputasi.\n",
        "\n",
        "   b. `outputs = model(inputs)`: Mendapatkan prediksi dari model untuk batch saat ini.\n",
        "\n",
        "   c. `_, predictions = torch.max(outputs, 1)`: Menggunakan `torch.max()` untuk mendapatkan indeks kelas yang memiliki probabilitas tertinggi. Dalam hal ini, yang diambil adalah indeks dari kelas dengan nilai probabilitas terbesar.\n",
        "\n",
        "   d. `(predictions == labels).sum().item()`: Menghitung jumlah prediksi yang benar dengan membandingkan prediksi model dengan label yang sebenarnya, kemudian menjumlahkannya. `.sum()` digunakan untuk menghitung jumlah elemen True (prediksi yang benar), dan `.item()` mengambil nilai hasilnya.\n",
        "\n",
        "   e. `total_correct += (predictions == labels).sum().item()`: Menambahkan jumlah prediksi yang benar dari batch saat ini ke total_correct.\n",
        "\n",
        "   f. `total_samples += labels.size(0)`: Menambahkan jumlah sampel dalam batch saat ini ke total_samples.\n",
        "\n",
        "5. `accuracy = total_correct / total_samples`: Menghitung akurasi model dengan membagi jumlah prediksi yang benar dengan jumlah total sampel yang dievaluasi.\n",
        "\n",
        "6. `print(f\"Test Accuracy: {accuracy}\")`: Mencetak akurasi model pada dataset pengujian.\n",
        "\n",
        "Proses ini merupakan tahap evaluasi setelah pelatihan selesai. Dengan menggunakan dataset pengujian yang terpisah, Anda dapat mengevaluasi seberapa baik model Anda dapat melakukan prediksi pada data baru yang tidak pernah dilihat selama proses pelatihan. Output yang dihasilkan adalah akurasi model pada dataset pengujian, yang memberikan gambaran tentang seberapa baik model dapat menggeneralisasi pada data yang belum pernah dilihat sebelumnya."
      ],
      "metadata": {
        "id": "gSX_BtSvOpJw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing\n",
        "model.eval()\n",
        "total_correct = 0\n",
        "total_samples = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        _, predictions = torch.max(outputs, 1)\n",
        "\n",
        "        total_correct += (predictions == labels).sum().item()\n",
        "        total_samples += labels.size(0)\n",
        "\n",
        "accuracy = total_correct / total_samples\n",
        "print(f\"Test Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBEL1ardHxFz",
        "outputId": "d4a0bb0f-1c8c-44b6-c8c2-eac63098b76b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.9919\n"
          ]
        }
      ]
    }
  ]
}